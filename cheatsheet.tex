\documentclass[10pt,landscape,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{tikz}
\usetikzlibrary{shapes,positioning,arrows,fit,calc,graphs,graphs.standard}
\usepackage[nosf]{kpfonts}
\usepackage[t1]{sourcesanspro}
%\usepackage[lf]{MyriadPro}
%\usepackage[lf,minionint]{MinionPro}
\usepackage{multicol}
\usepackage{wrapfig}
\usepackage[top=1mm,bottom=2mm,left=2mm,right=1mm]{geometry}
\usepackage[framemethod=tikz]{mdframed}
\usepackage{microtype}
\usepackage{hyperref}

\usepackage{url}
\usepackage{multirow}
\usepackage{esint}
\usepackage{amsfonts}
\usetikzlibrary{decorations.pathmorphing}

\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{enumitem}
\makeatletter

\let\bar\overline

\setlist[itemize]{topsep=0pt,leftmargin=15pt,itemsep=-0.2em}
\definecolor{myblue}{cmyk}{1,.72,0,.38}
\definecolor{mypurple}{cmyk}{.57,1,0,.58}
\definecolor{myred}{cmyk}{0,.88,.88,.58}
\definecolor{mygreen}{cmyk}{1,0,.69,.66}
\definecolor{myorange}{cmyk}{0,.58,100,.20}

\def\firstcircle{(0,0) circle (1.5cm)}
\def\secondcircle{(0:2cm) circle (1.5cm)}

\pgfdeclarelayer{background}
\pgfsetlayers{background,main}

\renewcommand{\baselinestretch}{.8}
\pagestyle{empty}

\global\mdfdefinestyle{header}{%
linecolor=gray,linewidth=1pt,%
leftmargin=0mm,rightmargin=0mm,skipbelow=0mm,skipabove=0mm,
}

%\newcommand{\header}{
%\begin{mdframed}[style=header]
%\footnotesize
%\sffamily
%Cheat sheet\\
%by~Your~Name,~page~\thepage~of~2
%\end{mdframed}
%}

\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}{1ex}{.2ex}{\normalsize\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{1}{0mm}{.2ex}{.2ex}{\bfseries}}

\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

\def\multi@column@out{%
   \ifnum\outputpenalty <-\@M
   \speci@ls \else
   \ifvoid\colbreak@box\else
     \mult@info\@ne{Re-adding forced
               break(s) for splitting}%
     \setbox\@cclv\vbox{%
        \unvbox\colbreak@box
        \penalty-\@Mv\unvbox\@cclv}%
   \fi
   \splittopskip\topskip
   \splitmaxdepth\maxdepth
   \dimen@\@colroom
   \divide\skip\footins\col@number
   \ifvoid\footins \else
      \leave@mult@footins
   \fi
   \let\ifshr@kingsaved\ifshr@king
   \ifvbox \@kludgeins
     \advance \dimen@ -\ht\@kludgeins
     \ifdim \wd\@kludgeins>\z@
        \shr@nkingtrue
     \fi
   \fi
   \process@cols\mult@gfirstbox{%
%%%%% START CHANGE
\ifnum\count@=\numexpr\mult@rightbox+2\relax
          \setbox\count@\vsplit\@cclv to \dimexpr \dimen@-1cm\relax
\setbox\count@\vbox to \dimen@{\vbox to 1cm{\header}\unvbox\count@\vss}%
\else
      \setbox\count@\vsplit\@cclv to \dimen@
\fi
%%%%% END CHANGE
            \set@keptmarks
            \setbox\count@
                 \vbox to\dimen@
                  {\unvbox\count@
                   \remove@discardable@items
                   \ifshr@nking\vfill\fi}%
           }%
   \setbox\mult@rightbox
       \vsplit\@cclv to\dimen@
   \set@keptmarks
   \setbox\mult@rightbox\vbox to\dimen@
          {\unvbox\mult@rightbox
           \remove@discardable@items
           \ifshr@nking\vfill\fi}%
   \let\ifshr@king\ifshr@kingsaved
   \ifvoid\@cclv \else
       \unvbox\@cclv
       \ifnum\outputpenalty=\@M
       \else
          \penalty\outputpenalty
       \fi
       \ifvoid\footins\else
         \PackageWarning{multicol}%
          {I moved some lines to
           the next page.\MessageBreak
           Footnotes on page
           \thepage\space might be wrong}%
       \fi
       \ifnum \c@tracingmulticols>\thr@@
                    \hrule\allowbreak \fi
   \fi
   \ifx\@empty\kept@firstmark
      \let\firstmark\kept@topmark
      \let\botmark\kept@topmark
   \else
      \let\firstmark\kept@firstmark
      \let\botmark\kept@botmark
   \fi
   \let\topmark\kept@topmark
   \mult@info\tw@
        {Use kept top mark:\MessageBreak
          \meaning\kept@topmark
         \MessageBreak
         Use kept first mark:\MessageBreak
          \meaning\kept@firstmark
        \MessageBreak
         Use kept bot mark:\MessageBreak
          \meaning\kept@botmark
        \MessageBreak
         Produce first mark:\MessageBreak
          \meaning\firstmark
        \MessageBreak
        Produce bot mark:\MessageBreak
          \meaning\botmark
         \@gobbletwo}%
   \setbox\@cclv\vbox{\unvbox\partial@page
                      \page@sofar}%
   \@makecol\@outputpage
     \global\let\kept@topmark\botmark
     \global\let\kept@firstmark\@empty
     \global\let\kept@botmark\@empty
     \mult@info\tw@
        {(Re)Init top mark:\MessageBreak
         \meaning\kept@topmark
         \@gobbletwo}%
   \global\@colroom\@colht
   \global \@mparbottom \z@
   \process@deferreds
   \@whilesw\if@fcolmade\fi{\@outputpage
      \global\@colroom\@colht
      \process@deferreds}%
   \mult@info\@ne
     {Colroom:\MessageBreak
      \the\@colht\space
              after float space removed
              = \the\@colroom \@gobble}%
    \set@mult@vsize \global
  \fi}

\hypersetup{
    colorlinks=true,
    linkcolor=myblue,
    filecolor=magenta,      
    urlcolor=myblue,
    pdfpagemode=FullScreen,
    }

\urlstyle{same}

\makeatother
\setlength{\parindent}{0pt}

% material references: Prof. Geert Ridder's lecture notes
% latex coding references: https://github.com/tim-st/latex-cheatsheet, https://www.overleaf.com/latex/templates/hoja-de-ecuaciones-electricidad-y-magnetismo/xwgjqkrjjgcb

\begin{document}
\begin{center}{\large{\textbf{Probability and Statistics for Economics Cheat Sheet}}}\\
Author: Sai Zhang (\href{mailto:saizhang.econ@gmail.com}{email} me or check my \href{https://github.com/SaiChrisZHANG}{Github} page)
\end{center}

\small
\begin{multicols*}{5}

% set box styles: in this file, I only use red blue and purple boxes
%% blue boxes
\tikzstyle{bluebox} = [draw=myblue, fill=white, thick, rectangle, rounded corners, inner sep=5pt, inner ysep=10pt, text=myblue]
\tikzstyle{bluetitle} =[fill=myblue, text=white, font=\bfseries]
\tikzstyle{ibluebox} = [draw=myblue, fill=myblue, thick, rectangle, rounded corners, inner sep=5pt, inner ysep=10pt, text=white]
\tikzstyle{ibluetitle} =[draw=myblue, fill=white, text=myblue, font=\bfseries]
%% red boxes
\tikzstyle{redbox} = [draw=myred, fill=white, thick, rectangle, rounded corners, inner sep=5pt, inner ysep=10pt, text=myred]
\tikzstyle{redtitle} =[fill=myred, text=white, font=\bfseries]
\tikzstyle{iredbox} = [draw=myred, fill=myred, thick, rectangle, rounded corners, inner sep=5pt, inner ysep=10pt, text=white]
\tikzstyle{iredtitle} =[draw=myred, fill=white, text=myred, font=\bfseries]
%% purple boxes
\tikzstyle{purplebox} = [draw=mypurple, fill=white, thick, rectangle, rounded corners, inner sep=5pt, inner ysep=10pt, text=mypurple]
\tikzstyle{purpletitle} =[fill=mypurple, text=white, font=\bfseries]
\tikzstyle{ipurplebox} = [draw=mypurple, fill=mypurple, thick, rectangle, rounded corners, inner sep=5pt, inner ysep=10pt, text=white]
\tikzstyle{ipurpletitle} =[draw=mypurple, fill=white, text=mypurple, font=\bfseries]
%% orange boxes
\tikzstyle{orangebox} = [draw=myorange, fill=white, thick, rectangle, rounded corners, inner sep=5pt, inner ysep=10pt, text=myorange]
\tikzstyle{orangetitle} =[fill=myorange, text=white, font=\bfseries]
\tikzstyle{iorangebox} = [draw=myorange, fill=myorange, thick, rectangle, rounded corners, inner sep=5pt, inner ysep=10pt, text=white]
\tikzstyle{iorangetitle} =[draw=myorange, fill=white, text=myorange, font=\bfseries]

\section*{Random experiments}
The outcome in a random experiment is \textbf{unpredictable}:
\begin{itemize}
    \item[-] outcome is too complicates or poorly understood
    \item[-] outcome is designed to be unpredictable
    \item[-] coincidences, or independent chains of events
\end{itemize}

\vspace{5pt}
\begin{tikzpicture}
\node [orangebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \footnotesize
    \textbf{Random card shuffle experiment}: take top card from a deck and insert randomly, to complete the shuffle of $n$ cards, we need $$T=n+\frac{n}{2}+\cdots +\frac{n}{n-1}+1= n\log n$$ shuffles.
    
    \textbf{Random number generator}: $$x_{n+1}=\frac{ax_n+b}{c}-\left[\frac{ax_n+b}{c}\right]$$the remainder after dividing by $c$, hence $x_{n+1}\in [0,c-1]$, let $u_{n+1}=\frac{x_{n+1}}{c}$, $x_0,a,b,c$ all be integers. For very large $a$ and good choice of $b,c$, the sequence $u_1,u_2,\cdots$ is like a sequence of numbers randomly picked from $[0,1]$
    \end{minipage}
};
\node[orangetitle, right=4pt] at (box.north west) {Two examples};
\end{tikzpicture}

\section*{Probabilities}
Probability is a number in $[0,1]$ that measures the likelihood of an outcome or a set of outcomes.

Ways of assigning probabilities:
\begin{itemize}
    \item[-] \textbf{symmetry}: assume all outcomes are equally likely
    \item[-] \textbf{experimental method}: relative frequency in repeated random experiment
    \item[-] \textbf{subjective method}: assign probabilities using knowledge of random experiment
    \item[-] \textbf{market method}
\end{itemize}

\vspace{5pt}
\begin{tikzpicture}
\node [bluebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myblue}
     \begin{itemize}
        \item[-] \textbf{outcome space} $\Omega$ and outcomes $\omega \in \Omega$
        \item[-] \textbf{event} $E$, $E\subset \Omega$
        \item[-] \textbf{probability function/measure} $P:\mathcal{A}\rightarrow [0,1]$: a function from \textbf{a collection} $\mathcal{A}$ of subsets of $\Omega$ to the interval $\mathbf{[0,1]}$.
    \end{itemize}
    \end{minipage}
};
\node[bluetitle, right=4pt] at (box.north west) {Elements of probability space};
\end{tikzpicture}

\section*{Classes of events}
Events $E_1,E_2,\cdots$ are just sets. They also follow the algebras of sets.

\begin{tikzpicture}
\node [orangebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \footnotesize
    $A\cup (B\cap C)=(A\cup B)\cap (A\cup C)$
    
    $A\cap (B\cup C)=(A\cap B)\cup (A\cap C)$
    
    $A\cup A^C=U$, $ A\setminus B=A\cap B^C$
    
    $(\bigcup_{i=1}^{\infty}A_i)^C=\bigcap_{i=1}^{\infty}A_i^C$
    
    $(\bigcap_{i=1}^{\infty}A_i)^C=\bigcup_{i=1}^{\infty}A_i^C$
    
    $A\cup B=U,A\cap B=\varnothing\Leftrightarrow B=A^C$
    
    $(A^C)^C=A$
    
    $A\subseteq B\Leftrightarrow A\cap B=A\Leftrightarrow A\cup B=B\Leftrightarrow A\setminus B=\varnothing \Leftrightarrow B^C\subseteq A^C$
    \end{minipage}
};
\node[orangetitle, right=4pt] at (box.north west) {Some set algebras};
\end{tikzpicture}

Two special relations:
\begin{itemize}
    \item[-] \textbf{disjoint}: $E_1\cap E_2=\varnothing$
    \item[-] \textbf{partition}: $\bigcup^{\infty}_{i=1}E_i=\Omega$, $\{E_i\}$ are pairwise disjoint
\end{itemize}

\section*{$\sigma-$field and Borel $\sigma-$field}

\vspace{5pt}
\begin{tikzpicture}
\node [bluebox] (box){%
    \color{myblue}
    \begin{minipage}{0.18\textwidth}
     $\mathcal{A}$ (a collection of subsets of $\Omega$) is a $\sigma-$field if:
\begin{itemize}
    \item[\textbf{1}] $\varnothing \in \mathcal{A}$
    \item[\textbf{2}] $E\in \mathcal{A}\Rightarrow E^C\in \mathcal{A}$
    \item[\textbf{3}] $E_1,E_2,\cdots \in \mathcal{A}\Rightarrow \bigcup^{\infty}_{i=1}E_i\in\mathcal{A}$
\end{itemize}
It is easy to see that:
$\Omega\in\mathcal{A}$ (by 1), $\bigcap E^C_i \in\mathcal{A}$, $\bigcup E^C_i\in\mathcal{A}$, $\bigcap E_i\in\mathcal{A}$ as well
\end{minipage}
};
\node[bluetitle, right=4pt] at (box.north west) {Definition of $\sigma-$field};
\end{tikzpicture}
    
Two important $\sigma-$field:
\begin{itemize}
    \item[-] Trivial $\sigma-$field:$\mathcal{A}=\{\varnothing,\Omega\}$
    \item[-] Largest $\sigma-$field: \textbf{powerset} of $\Omega$, $\mathcal{P}(\Omega)$
\end{itemize}

\subsection*{Generating classes}
When the powerset has too many events to be assigned probabilities to each event, we start from a set of events $\mathcal{E}$ that we want to assign probabilities to. This $\mathcal{E}$ is a \textbf{generating class}.

 \vspace{2pt}
\begin{tikzpicture}
\node [bluebox] (box){%
    \color{myblue}
    \begin{minipage}{0.18\textwidth}
    The \textbf{smallest} $\sigma-$field that contains $\mathcal{E}$ is
    $$\sigma(\mathcal{E})=\{E\subseteq \Omega\mid E\in\mathcal{A},\forall \mathcal{A}\supseteq \mathcal{E} \}$$
    where $\mathcal{A}$ can be any $\sigma-$field (including the powerset). 
    
    Another way to write this definition is: for all the $\sigma-$fields that contain $\mathcal{E}$, $\{\mathcal{A}_i\mid \mathcal{E}\subseteq \mathcal{A}_i\}$, we have:
    $$\sigma(\mathcal{E})=\bigcap \mathcal{A}_i$$
\end{minipage}
};
\node[bluetitle, right=4pt] at (box.north west) {$\sigma-$field generated by $\mathcal{E}$};
\end{tikzpicture}

This definition is based on the fact: \textbf{any intersection of $\sigma-$fields is a $\sigma-$field}, here is a proof:

 \vspace{2pt}
\begin{tikzpicture}
\node [redbox] (box){%
    \color{myred}
    \begin{minipage}{0.18\textwidth}
     We can check the three definitions of $\sigma-$field: for $\mathcal{F}_1,\mathcal{F}_2$
     \begin{itemize}
         \item[1] $\varnothing\in\mathcal{F}_1,\varnothing\in\mathcal{F}_2\Rightarrow \varnothing\in\mathcal{F}_1\cap \mathcal{F}_2$ 
         \item[2] $E\in\mathcal{F}_1\cap\mathcal{F}_2\Rightarrow E^C\in\mathcal{F}_1,E^C\in\mathcal{F}_2\Rightarrow E^C\in\mathcal{F}_1\cap\mathcal{F}_2$
         \item[3] $E_1,\cdots\in\mathcal{F}_1\cap\mathcal{F}_2\Rightarrow E_1,\cdots \in\mathcal{F}_1,E_1,\cdots\in\mathcal{F}_2\Rightarrow \Rightarrow \bigcup E_i\in \mathcal{F}_1,\bigcup E_i\in\mathcal{F}_2\Rightarrow \bigcup E_i \in \mathcal{F}_1\cap \mathcal{F}_2$
     \end{itemize}
\end{minipage}
};
\node[redtitle, right=4pt] at (box.north west) {Proof: $\cap$ of two $\sigma-$fields is a $\sigma-$field};
\end{tikzpicture}

\subsection*{Borel $\sigma-$field}
Borel $\sigma-$field $\mathcal{B}$ is a $\sigma-$field on $\mathbb{R}$, its generating class is the set of all \textbf{open subsets} of $\mathbb{R}$. But $\mathcal{B}$'s generating class is not unique, a particularly important one is $\mathcal{E}=\{\left(-\infty,x\right]\mid x\in\mathbb{R}\}$. Here is a proof of this generating class can actually generate $\mathcal{B}$.

 \vspace{2pt}
\begin{tikzpicture}
\node [redbox] (box){%
    \color{myred}
    \begin{minipage}{0.18\textwidth}
     The proof is done in 2 steps:
     \begin{itemize}
         \item[-] prove $\sigma(\mathcal{E})\subseteq \mathcal{B}$:
        $\mathcal{E}=\left(-\infty,x\right]=\bigcap^{\infty}_{n=1}\left(-\infty, x+\frac{1}{n}\right)\in\mathcal{B}$. And by the definition of $\sigma-$field generated by $\mathcal{E}$, we know that $\sigma(\mathcal{E})=\bigcap\mathcal{A}_i\subseteq \mathcal{E}$, hence $\sigma(\mathcal{E})\subseteq \mathcal{B}$
         
         \item[-] prove $\mathcal{B}\subseteq \sigma(\mathcal{E})$: each open set $B$ in $\mathbb{R}$ can be written as $B=\cup^{\infty}_{i=1}\left(a_i,b_i\right)$. For each $(a,b)$, we can rewrite it as $(a,b)=(-\infty,b)\cap(-\infty,a]^C$, where $(-\infty,b)=\bigcup^{\infty}_{n=1}\left(\infty,b-\frac{1}{n}\right]$. By the definition of $\sigma-$field, $\bigcup^{\infty}_{n=1}\left(\infty,b-\frac{1}{n}\right]\cap(-\infty,a]^C\in \sigma(\mathcal{E})$, hence $B\in\sigma(\mathcal{E}).$
     \end{itemize}
     
     This is actually a special case of \textbf{generating class arguments}, which is used to show that all sets in a $\sigma-$field $\mathcal{A}$ have a certain property. It follows:
     \begin{itemize}
         \item[-] for subsets of $\Omega$ that have the property, define the collection of them as $\mathcal{E}$
         \item[-] show that $\mathcal{A}\subseteq \sigma(\mathcal{E})$
         \item[-] show that for $\mathcal{A}_0=\left\{A_{\text{property}}\in\mathcal{A}\right\}$ with $\mathcal{E}\subseteq \mathcal{A}_0$, $\mathcal{A}_0$ is a sigma field (Since $\sigma(\mathcal{E})$ is the smallest $\sigma-$field that contains $\mathcal{E}$, $\sigma(\mathcal{E})\subseteq \mathcal{A}_0$)
         \item[-] $\mathcal{A}\subseteq \sigma(\mathcal{E})\subseteq \mathcal{A}_0\subseteq \mathcal{A}\Rightarrow \mathcal{A}=\mathcal{A}_0$
     \end{itemize}
\end{minipage}
};
\node[redtitle, right=4pt] at (box.north west) {Proof: $\sigma(\mathcal{E})=\mathcal{B}$};
\end{tikzpicture}
Often it is difficult to show that $\mathcal{A}_0$ is a $\sigma-$field, hence we introduce $\lambda-$systems.

\subsection*{$\lambda-$systems}
$\mathcal{A}_0$ is a $\lambda-$system if
\begin{itemize}
    \item[1] $\Omega\in\mathcal{A}_0$
    \item[2] If $D_1,D_2\in\mathcal{A}_0$ and $D_2\subseteq D_1$, then $D_1\setminus D_2 =D_1\cap D_2^C\in\mathcal{A}_0$
    \item[3] If $D_n$ is an increasing sequence of sets in $\mathcal{A}_0$, then $\bigcup^{\infty}_{i=1}D_i\in\mathcal{A}_0$
\end{itemize}

Two theorems link $\lambda-$systems and $\sigma-$fields:

 \vspace{2pt}
\begin{tikzpicture}
\node [ibluebox] (box){%
    \color{white}
    \begin{minipage}{0.18\textwidth}
    \begin{itemize}
        \item[-] \textbf{Theorem 1}: If $\mathcal{E}$ is closed under finite intersections, and if $\mathcal{A}_0$ is a $\lambda-$system with $\mathcal{E}\subseteq \mathcal{A}_0$, then $\sigma(\mathcal{E})\subseteq \mathcal{A}_0$.
        \item[-] \textbf{Theorem 2}: a $\lambda-$system $\mathcal{A}_0$ is a $\sigma-$field $\Leftrightarrow$ $\mathcal{A}_0$ is closed in finite intersections.
    \end{itemize}
\end{minipage}
};
\node[ibluetitle, right=4pt] at (box.north west) {Theorems of $\lambda-$systems/$\sigma-$fields};
\end{tikzpicture}

\subsection*{Choice of $\sigma-$field}
The choice of $\sigma-$field is usually determined by the nature of the outcome space:
\begin{itemize}
    \item[-] discrete (countable) outcome space: \textbf{powerset of $\Omega$} shall be chosen.
    \item[-] continuous outcome space (real line): \textbf{Borel $\sigma-$field} shall be chosen.
\end{itemize}

\section*{Probability measure}
\begin{tikzpicture}
\node [bluebox] (box){%
    \begin{minipage}{0.18\textwidth}
     A probability measure is a function $P:\mathcal{A}\Rightarrow\mathbb{R}$ with a $\sigma-$field $\mathcal{A}$:
\begin{itemize}
    \item[\textbf{1}] $\forall E \in \mathcal{A},P(E)\geq 0$
    \item[\textbf{2}] $P(\Omega)=1$
    \item[\textbf{3}] If $E_1,E_2,\cdots$ are pairwise disjoint (and countable), $P\left(\bigcup^{\infty}_{i=1}E_i\right)=\sum^{\infty}_{i=1}P(E_i)$
\end{itemize}
    \end{minipage}
};
\node[bluetitle, right=4pt] at (box.north west) {Definition of probability measure};
\end{tikzpicture}
The usual way to specify probabilities using a probability model is to first assign probabilities to some simple collection of events $\mathcal{E}$, these assignments can be extended to $\sigma(\mathcal{E})$, this extension can usually be shown to be unique, giving a probability measure on $\sigma(\mathcal{E})$.

 \vspace{2pt}
\begin{tikzpicture}
\node [orangebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myorange}
    \footnotesize
    For the problem of rolling a single dice, the probability measure can be defined as
    \begin{itemize}
        \item[-] $\sigma-$field $\mathcal{A}$: powerset of $\Omega$
        \item[-] probability measure: $P(E)=\frac{\#E}{6}$
    \end{itemize}
    \end{minipage}
};
\node[orangetitle, right=4pt] at (box.north west) {Discrete $\Omega$: Dice roll};
\end{tikzpicture}
This construction can be extended to all discrete outcome spaces: starting by assigning probabilities $p_i$ to single outcomes and define $P(E)=\sum_{i\in E}p_i$.

Here is a special case of discrete $\Omega$: random experiments with equally likely outcomes, i.e., $\Omega=\{\omega_i\}_{i=1}^I,P(\omega_i)=\frac{1}{I}, P(E)=\frac{\#E}{I}$.

 \vspace{2pt}
\begin{tikzpicture}
\node [orangebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myorange}
    \footnotesize
    Consider the problem of selecting $k$ elements from $n$ distinct elements, the number of selections $N_s$ has 4 cases:
    \begin{itemize}
        \item[-] Ordered without replacement: $N_s=\frac{n!}{(n-k)!}$
        \item[-] Ordered with replacement: $N_s=n^k$
        \item[-] Unordered without replacement: $N_s=\frac{n!}{k!(n-k)!}=\begin{pmatrix}n\\k \end{pmatrix}$
        \item[-] Unordered with replacement: $S_n=\begin{pmatrix}n+k-1\\k \end{pmatrix}$
    \end{itemize}
    \end{minipage}
};
\node[orangetitle, right=4pt] at (box.north west) {4 cases of equally likely $\Omega$};
\end{tikzpicture}

 \vspace{2pt}
\begin{tikzpicture}
\node [bluebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myblue}
    \footnotesize
    For the probability measure on $\mathcal{B}$ can be defined as
    \begin{itemize}
        \item[-] probabilities on $\mathcal{E}=\left\{\left(-\infty,x\right]\mid x\in\mathbb{R} \right\}$
    \end{itemize}
    \end{minipage}
};
\node[bluetitle, right=4pt] at (box.north west) {Continuous $\Omega$: Borel $\sigma-$field $\mathcal{B}$};
\end{tikzpicture}

$\mathcal{E}=\left\{\left(-\infty,x\right]\mid x\in\mathbb{R} \right\}$ is closed under finite intersection, hence, we need to show $\forall B\in\mathcal{B}$, $P(B)=P'(B)$, that is, $\mathcal{E}$ has a unique probability measure. 

Consider $\mathcal{B}_0=\left\{B\in\mathcal{B}\mid P(B)=P'(B)\right\}$, which is a $\lambda-$system. For an increasing sequence $B_n$ of events in $\mathcal{B}_0$, $P(B)=P(B_1\cup(B_2\setminus B_1)\cup\cdots\cup (B_n\setminus B_{n-1})=P(B_1)+(P(B_2)-P(B_1))+\cdots+(P(B_n)-P(B_{n-1}))=\lim P(B_n)$ then  $P(B)=\lim P(B_n)=\lim P'(B_n)=P'(B)$. Now $\mathcal{E}\subseteq \mathcal{B}_0$ is closed under finite intersections, then by Thm.1 of $\lambda-$system, $\sigma(\mathcal{E})\subseteq \mathcal{B}_0\subseteq\mathcal{B}$, then $\mathcal{B}=\sigma(\mathcal{E})$, hence $\forall B\in\mathcal{B},P(B)=P'(B)$.

 \vspace{2pt}
\begin{tikzpicture}
\node [orangebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myorange}
    \footnotesize
    \begin{itemize}
        \item[-] $P(E^C)=1-P(E)$, $P(\varnothing)=0$, $P(E)\leq 1$
        \item[-] $P(E_2\setminus E_1)=P(E_2\cap E_1^C)=P(E_2)-P(E_2\cap E_1)$
        \item[-] $P(E_1\cup E_2)=P(E_1\cup(E_2\setminus E_1))=P(E_1)+P(E_2)-P(E_1\cap E_2)$
        \item[-] $E_1\subseteq E_2\Rightarrow E_2=E_1\cup(E_2\setminus E_1)\Rightarrow P(E_1)\leq P(E_2)$
        \item[-] $P(E_1\cup E_2)\leq P(E_1)+P(E_2)$
        \item[-] Bonferroni inequality: $P(E_2\cap E_1)\geq P(E_1)+P(E_2)-1$
        \item[-] Law of total probability: for a partition of $\Omega$, $\{E_i\}$, $P(A)=\sum^{\infty}_{i=1}P(A\cap E_i)$
        \item[-] Boole's inequality: $P(\bigcup^{\infty}_{i=1}E_i)\leq \sum^{\infty}_{i=1}P(E_i)$
    \end{itemize}
    The proof of all these results relies on the step to transform the object set to a \textbf{union of pairwise disjointed} sets.
    \end{minipage}
};
\node[orangetitle, right=4pt] at (box.north west) {Properties of probability measures};
\end{tikzpicture}

For all subsets of $\mathbb{R}$, it is impossible to assign probabilities to them. The proof takes advantage of a series of uncountable, disjointed subsets centered at irrational numbers.

\section*{Random variable}

 \vspace{2pt}
\begin{tikzpicture}
\node [bluebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myblue}
    \footnotesize
    For a probability space $(\Omega,\mathcal{A},P)$, a random varialbe $X$ is a function $X:\Omega\rightarrow \mathbb{R}$ s.t. $\forall B\in$ Borel $\sigma-$field $\mathcal{B}$, $E=\left\{\omega\mid X(\omega)\in B\right\}\in \mathcal{A}$.
    \end{minipage}
};
\node[bluetitle, right=4pt] at (box.north west) {Definition of random varibles};
\end{tikzpicture}

Another way of stating it is: $X$ takes a value in $B\Leftrightarrow w\in E$ i.e., event E happens. Thus, $\Pr(X\in B)=\Pr(E)$

 \vspace{2pt}
\begin{tikzpicture}
\node [ibluebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{white}
    \footnotesize
    \begin{itemize}
        \item[-] $E=X^{-1}(B)$ ($X^{-1}$ does \textbf{NOT} necessarily exist)
        \item[-] $X$ is \textbf{Borel measurable}
    \end{itemize}
    \end{minipage}
};
\node[ibluetitle, right=4pt] at (box.north west) {2 things to keep in mind};
\end{tikzpicture}

 \vspace{2pt}
\begin{tikzpicture}
\node [orangebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myorange}
    \footnotesize
    \begin{itemize}
        \item[-] $X^{-1}(B^C)=(X^{-1}(B))^C$
        \item[-] $X^{-1}\left(\bigcup^{\infty}_{i=1}B_i\right)=\bigcup^{\infty}_{i=1}X^{-1}(B_i)$
    \end{itemize}
    \end{minipage}
};
\node[orangetitle, right=4pt] at (box.north west) {Properties of $X^{-1}$};
\end{tikzpicture}

\subsection*{Borel Measurability}
Borel measurability of a random variable $X$ (function) is established separately by the countability of outcome space $\Omega$.

\vspace{2pt}
\begin{tikzpicture}
\node [redbox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myred}
    \footnotesize
    For a countable $\Omega$, $X:\Omega \rightarrow \mathbb{R}$ is Borel measurable if $\sigma-$field on $\Omega$, $\mathcal{A}$, is the \textbf{powerset} of $\Omega$
    \end{minipage}
};
\node[redtitle, right=4pt] at (box.north west) {Countable $\Omega$};
\end{tikzpicture}

\vspace{2pt}
\begin{tikzpicture}
\node [redbox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myred}
    \footnotesize
    Let $\mathcal{E}\subseteq \mathcal{B}$ be a generating class of Borel $\sigma-$field $\mathcal{B}$, for an uncountable $\Omega$ and its $\sigma-$field $\mathcal{A}$, $X$ is Borel measurable if $\forall E\in\mathcal{E}, X^{-1}(E)\in\mathcal{A}$. 
    
    We use a \textbf{generating class argument} to prove this:
    
    \textbf{Step 1}: Define $\mathcal{C}=\left\{B\in\mathcal{B}\mid X^{-1}(B)\in\mathcal{A} \right\}$, since $\forall E\in\mathcal{E}, X^{-1}(E)\in\mathcal{A}$, we know $\mathcal{E}\subseteq\mathcal{C}$.
    
    \textbf{Step 2}: Check $\mathcal{C}$ is a  $\sigma-$field:
    \begin{itemize}
        \item[i] $\varnothing \in \mathcal{C}$
        \item[ii] $B\in\mathcal{C}\Rightarrow X^{-1}(B)\in\mathcal{A}\Rightarrow X^{-1}(B^C)=X^{-1}(B)^C\in\mathcal{A}\Rightarrow B^C\in \mathcal{C}$
        \item[iii] $B_1,B_2,\cdots \in\mathcal{C}\Rightarrow X^{-1}(B_1),\cdots \in\mathcal{A}\Rightarrow X^{-1}(\bigcup^{\infty}_{i=1}B_i)=\bigcup^{\infty}_{i=1}X^{-1}(B_i)\in \mathcal{A}$
    \end{itemize}
    \textbf{Step 3}: Since $\mathcal{B}$ is the smallest $\sigma-$field containing $\mathcal{E}$ (definition of generating class), $\mathcal{C}$ is a $\sigma-$field containing $\mathcal{E}$, $\mathcal{B}\subseteq \mathcal{C}$; by the definition of $\mathcal{C}$, $\mathcal{C}\subseteq \mathcal{B}$. Hence $\mathcal{C}=\mathcal{B}$, meaning that $\forall B\in\mathcal{B},X^{-1}(B)\in\mathcal{A}$, and $X$ is Borel measurable.
    \end{minipage}
};
\node[redtitle, right=4pt] at (box.north west) {Uncountable $\Omega$};
\end{tikzpicture}

Borel measurability has some important applications:

 \vspace{2pt}
\begin{tikzpicture}
\node [orangebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myorange}
    \footnotesize
    For the probability space $(\mathbb{R},\mathcal{B},P)$ and a \textbf{continuous} $X:\mathbb{R}\rightarrow\mathbb{R}$ is Borel measurable.
    \end{minipage}
};
\node[orangetitle, right=4pt] at (box.north west) {Continuous $X:\mathbb{R}\rightarrow\mathbb{R}$};
\end{tikzpicture}

\vspace{2pt}
\begin{tikzpicture}
\node [orangebox](box){%
    \begin{minipage}{0.18\textwidth}
    \color{myorange}
    \footnotesize
    For a sequence of random variables, $\{X_n\}_{n=1}^{\infty}$, \textbf{$X_{\sup}=\sup_nX_n$ and $X_{\inf}=\inf_nX_n$ are also Borel measurable}.
    
    A brief proof:
    Take $(x,\infty)$ as a generating class for $\mathcal{B}$, $X_n$ being random variables $\Rightarrow\left\{w\mid X_n(w)>x\right\}\in\mathcal{A}\Rightarrow \bigcup_n\left\{w\mid X_n(w)>x\right\}= \left\{w\mid X_{\sup}(w)>x\right\}\in \mathcal{A}$. Taking $(-\infty,x]$ as the generating class can proof for $X_{\inf}$.
    \end{minipage}
};
\node[orangetitle, right=4pt] at (box.north west) {$X_{\sup}$ and $X_{\inf}$ of $X_n$};
\end{tikzpicture}

\vspace{2pt}
\begin{tikzpicture}
\node [orangebox](box){%
    \begin{minipage}{0.18\textwidth}
    \color{myorange}
    \footnotesize
    Again, for a sequence of random variables, $\{X_n\}_{n=1}^{\infty}$, \textbf{if $\lim_{n\rightarrow\infty}X_n=X$ exists, $X$ is a random variable}.
    
    A brief proof: we know
    \begin{align*}
    {\lim\inf}_{n\rightarrow\infty}X_n(\omega) &=\sup_n\inf_{m\geq n}X_m(\omega) \\
    {\lim\sup}_{n\rightarrow\infty}X_n(\omega) &=\inf_n\sup_{m\geq n}X_m(\omega)
    \end{align*}
    then it is easy to see ${\lim\inf}_{n\rightarrow\infty}X_n\leq X=\lim_{n\rightarrow\infty}X_n\leq {\lim\sup}_{n\rightarrow\infty}X_n$. If $\lim_{n\rightarrow\infty} X_n=X$ exists, the three limits are all equal and all Borel measurable.
    \end{minipage}
};
\node[orangetitle, right=4pt] at (box.north west) {$\lim_{n\rightarrow\infty}X_n=X$};
\end{tikzpicture}

\vspace{2pt}
\begin{tikzpicture}
\node [orangebox](box){%
    \begin{minipage}{0.18\textwidth}
    \color{myorange}
    \footnotesize
    If $X,Y$ are random variables, \textbf{$Z=X+Y$ is Borel measurable}.
    
    A brief proof: take the generating class $\mathcal{E}=\left\{(x,\infty)\right\}$, then for $A=\left\{\omega\mid Z(\omega)>z\right\}$, we have $X(\omega)+Y(\omega)>z\Leftrightarrow X(\omega)>z-Y(\omega)$. 
    
    \textbf{Here is the trick}: We can always find a rational number $r$ s.t. $X(\omega)>r>z-Y(\omega)$, and $A=\bigcup_r \left(\left\{\omega\mid X(\omega)>r \right\}\cap \left\{\omega\mid Y(\omega)>z-r \right\} \right)$. Since rational numbers are \textbf{countable}, $X$ and $Y$ are both Borel measurable, $A$ is in $\mathcal{A}$, hence $Z$ is Borel measurable.
    \end{minipage}
};
\node[orangetitle, right=4pt] at (box.north west) {Algebras of random variables};
\end{tikzpicture}

For the collection of all Borel measurable functions $\mathcal{M}=\left\{X:\Omega\Rightarrow\mathbb{R}\right\}$ ($\mathcal{M}^+$ for non-negative functions), We focus on a special class:

\vspace{2pt}
\begin{tikzpicture}
\node [bluebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myblue}
    \footnotesize
    $$X(\omega)=\sum^n_i\alpha_i I_{A_i}(\omega)$$
    where 
    \begin{itemize}
        \item[-] $I_A$ is the indicator function of event $A$
        \item[-] $A_i\in \mathcal{A},i=1,\cdots,n$ are a partition of $\Omega$
        \item[-] $\alpha_i$ are constants
    \end{itemize}
    \end{minipage}
};
\node[bluetitle, right=4pt] at (box.north west) {Simple functions};
\end{tikzpicture}
Why are they important: each non-negative borel measurable function can be approximated by an \textbf{increasing sequence} of simple functions!

\vspace{2pt}
\begin{tikzpicture}
\node [ibluebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{white}
    \footnotesize
    For $\forall X\in\mathcal{M}^+$,
    $$X_n(\omega)=2^{-n}\sum^{4^n}_{i=1}I_{X\geq \frac{i}{2^n}}(\omega)$$
    satisfies $0\leq X_1(\omega)\leq\cdots\leq X_n(\omega)$ and $X_n(\omega)\rightarrow X(w),\forall \omega\in\Omega$.
    \end{minipage}
};
\node[ibluetitle, right=4pt] at (box.north west) {Theorem of simple functions};
\end{tikzpicture}

\vspace{2pt}
\begin{tikzpicture}
\node [redbox] (box){%
    \color{myred}
    \begin{minipage}{0.18\textwidth}
    \textbf{Step 1: increasing}
    
    Let $\sum^{4^n}_{i=1}I_{X\geq \frac{i}{2^n}}(\omega)=C$ then it is easy to show that $\sum^{4^{n+1}}_{i=1}I_{X\geq \frac{i}{2^{n+1}}}(\omega)\geq 2C$, hence $X_n=\frac{C}{2^n}$, $X_{n+1}\geq\frac{2C}{2^{n+1}}=X_n$
    
    \textbf{Step 2: approximation}
    There are two scenarios:
\begin{itemize}
    \item[(a)] $X(\omega)=\infty$. It will give $X_n(\omega)=2^{-n}\sum^{4^n}_{i=1}I_{X\geq \frac{i}{2^n}}(\omega)=2^n$, hence $\lim_{n\rightarrow\infty}X_n(\omega)=\infty=X$
    \item[(b)] $X(\omega)<\infty$. For sufficiently large $n$, we can find a $k\in\left\{0,1,\cdots,4^n-1\right\}$ such that $k2^{-n}\leq X <(k+1)2^{-n}$, and $X_n=k2^{-n}$. Hence $\mid X-X_n\mid\leq 2^{-n}\rightarrow0\Rightarrow X_n\rightarrow X$
\end{itemize}
\end{minipage}
};
\node[redtitle, right=4pt] at (box.north west) {Proof of $I_A(\omega)$ approximation};
\end{tikzpicture}

\section*{Expectation and integration}
Start from the simple case: $X(\omega)=\sum^n_{i=1}\alpha_i I_{A_i}(\omega)\Rightarrow E(X)=\sum^n_{i=1}\alpha_i P(A_i)\Rightarrow E(X)=\int_{\Omega}X(\omega)\mathrm{d}P(\omega)=\int X\mathrm{d}P$.

For general random variables, we take advantage of the \textbf{increasing sequences of simple function $X_n$ that asymptotically approaches} $X$.

\vspace{2pt}
\begin{tikzpicture}
\node [bluebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myblue}
    \footnotesize
    For simple functions $X_s$, $$E(X)=\int X\mathrm{d}P=\sup_{X_s}\left\{E(X_s)\mid X\geq X_s \right\}$$
    \end{minipage}
};
\node[bluetitle, right=4pt] at (box.north west) {Def. of $E(X)$: non-negative $X$};
\end{tikzpicture}

\vspace{2pt}
\begin{tikzpicture}
\node [orangebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myorange}
    \footnotesize
    \begin{itemize}
        \item[-] $\forall A\in\mathcal{A}, E(I_A)=P(A)$
        \item[-] $E(\mathbf{0}=0)$, where $\mathbf{0}$ is the null function that assigns 0 to all $\omega \in\Omega$
        \item[-] \textbf{linearity}: $\forall \alpha,\beta\geq 0$ and nonnegative Borel measurable functions $X,Y$ $$E(\alpha X+\beta Y)=\alpha E(X)+\beta E(Y)$$
        \item[-] If $\forall \omega\in\Omega, X(\omega)\leq Y(\omega)$, $E(Y)=E(X)+E(Y-X)\geq E(X)$
    \end{itemize}
    \end{minipage}
};
\node[orangetitle, right=4pt] at (box.north west) {Properties of $E(X)$};
\end{tikzpicture}

For \textbf{arbitrary} random variable $X$, we can write $X(\omega)=X_+(\omega)-X_-(\omega)$ with $X_+(\omega)=\max\left\{X(\omega),0\right\}$ and $X_-(\omega)=-\min\left\{X(\omega),0 \right\}$. Now $X_+$ and $X_-$ are both \textbf{non-negative}, and:

\vspace{2pt}
\begin{tikzpicture}
\node [bluebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myblue}
    \footnotesize
    For random variable $X=X_+-X_-$,  $$E(X)=E(X_+)-E(X_-)=\int_{\Omega}X_+\mathrm{d}P-\int_{\Omega}X_-\mathrm{d}P$$
    For $E(X)$ to be well-defined, we need $E(\mid X\mid)<\infty$, i.e., $X$ is \textbf{integrable}.
    \end{minipage}
};
\node[bluetitle, right=4pt] at (box.north west) {Def. of $E(X)$: arbitrary $X$};
\end{tikzpicture}

By introducing Jensen's inequality: $\forall \lambda\in(0,1)$, and a convex $f$:
$$f(\lambda x_1+(1-\lambda)x_2)\leq\lambda f(x_1)+(1-\lambda)f(x_2)$$
we have the following properties for a convex function $f$:

\vspace{2pt}
\begin{tikzpicture}
\node [orangebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myorange}
    \footnotesize
    For a convex $f:\mathbb{R}\rightarrow\mathbb{R}$, we have:
    \begin{itemize}
        \item[-] $f$ is \textbf{Borel measurable}: let $E=\left\{x\mid f(x)\in (-\infty,t] \right\}$, since $f$ is convex, $\forall x_1,x_2\in E,f(\lambda x_1+(1-\lambda)x_2)\leq \lambda f(x_1)+(1-\lambda)f(x_2)\leq t$. Hence, $E$ is an interval on $\mathbb{R}$, $f$ is Borel measurable.
        \item[-] $\forall x,x_0,f(x)\geq f(x_0)+\alpha(x-x_0)$ where $\alpha$ is a constant (may depend on $x_0$).
    \end{itemize}
    And, derive \textbf{two features of} $E(f(x))$:
    \begin{itemize}
        \item[-] $E(f(X)_-)<\infty$ if $E(\mid X\mid)<\infty$: we know $f(x)\geq f(x_0)+\alpha(x-x_0)\geq -|f(x_0)|-|\alpha|(|x|+|x_0|)$, hence $E(|X|)<\infty \Rightarrow E(f(X)_-) <E(|X|) <\infty$
        \item[-] Take $x_0=E(X)$ get $E(f(X))>f(E(X))+\alpha(E(X)-E(X))\Rightarrow E(f(X))\geq f(E(X))$
    \end{itemize}
    
    \end{minipage}
};
\node[orangetitle, right=4pt] at (box.north west) {$E(f(x))$ for a convex $f$};
\end{tikzpicture}

\subsection*{Lebesgue integrals}
In $E(X)=\int X\mathrm{d}P$, $P$ assigns probability 1 to $\Omega$. \textbf{Lebesgue measure} assigns probability 0 to $\varnothing$.

Lebesgue measure ($\Omega=\mathbb{R}$) is defined as $m([a,b])=b-a$ and Lebesgue measure of a point is 0. Hence $m((a,b))=b-a$. Open intervals $(a,b)$ are a generating class, $m$ can be uniquely extended to all sets in the Borel $\sigma-$field $\mathcal{B}$.

\textbf{Lebesgue integral} is then $\int^{\infty}_{-\infty}f(x)\mathrm{d}x$ (NOT $\int_{\mathbb{R}}f\mathrm{d}m$).

\vspace{2pt}
\begin{tikzpicture}
\node [bluebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myblue}
    \footnotesize
    The relation between Lebesgue integral and Riemann integral:
    \begin{itemize}
        \item[-] Same notation: $\int^{\infty}_{\infty}f(x)\mathrm{d}x$ 
        \item[-] If $f$ is integrable ($\int^{\infty}_{-\infty}|f(x)|\mathrm{d}x<\infty$), and Riemann integral exists, the two are the same.
        \item[-] If $\int^{\infty}_{-\infty}f(x)_+\mathrm{d}x=\int^{\infty}_{-\infty}f(x)_-\mathrm{d}x=\infty$, Lebesgue integral is NOT defined, but Riemann integral $\lim_{t\rightarrow\infty}\int^t_{-t}f(x)\mathrm{d}x$ may exist.
    \end{itemize}
    
    \end{minipage}
};
\node[bluetitle, right=4pt] at (box.north west) {Lebesgue and Riemann integral};
\end{tikzpicture}

The last group of properties of expectations regards convergence:

\vspace{2pt}
\begin{tikzpicture}
\node [ibluebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{white}
    \footnotesize
    Three properties are especially important:
    \begin{itemize}
        \item[(1)] \textbf{Monotone convergence}: if $X_n$ is an \textbf{increasing sequence of non-negative} random variables, i.e. $0\leq X_1\leq X_2\leq\cdots\leq X_n$, then 
        $$\lim_{n\rightarrow\infty}X_n=X\Rightarrow \lim_{n\rightarrow\infty}E(X_n)=E(X)$$
        \item[(2)] \textbf{Fatou's Lemma}: for $X_1,\cdots,X_n\geq 0$,
        $$E({\lim\inf}_{n\rightarrow\infty}X_n)\leq {\lim\inf}_{n\rightarrow\infty}E(X_n)$$
        \item[(3)] \textbf{Dominated convergence}: if $X_1,\cdots,X_n$ are integrable ($E(|X_i|)<\infty$), and there is a \textbf{non-negative, integrable} random variable $Y$ s.t. $|X_i(\omega)|\leq Y(\omega),\forall \omega\in\Omega,\forall i=1,\cdots,n$, then
        $$\lim_{n\rightarrow\infty}X_n=X\Rightarrow \lim_{n\rightarrow\infty}E(X_n)=E(X)$$
    \end{itemize}
    \end{minipage}
};
\node[ibluetitle, right=4pt] at (box.north west) {Convergence properties of $E(X)$};
\end{tikzpicture}

(1) is used to prove (2), (2) is used to prove (3):

\vspace{2pt}
\begin{tikzpicture}
\node [redbox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myred}
    \footnotesize
    \textbf{Monotone convergence}:
    
    \textbf{Fatou's Lemma}:
    
    \textbf{Dominated convergence}:
    \end{minipage}
};
\node[redtitle, right=4pt] at (box.north west) {Proof of $\rightarrow$ properties of $E(X)$};
\end{tikzpicture}

\subsection*{Sets of measure 0}
For integrals/expectations, the zero probability events $\left\{E\in\mathcal{A}\mid P(E)=0\right\}$ can be neglected. This gives:

\vspace{2pt}
\begin{tikzpicture}
\node [bluebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myblue}
    If random variables $X$ and $Y$ satisfy that for event $E=\left\{\omega\mid X(\omega)\neq Y(\omega)\right\}$, $P(E)=0$, then $E(X)=E(Y)$.
    \end{minipage}
};
\node[bluetitle, right=4pt] at (box.north west) {Expectation equality $E(X)=E(Y)$};
\end{tikzpicture}

This is very easy to prove:

\vspace{2pt}
\begin{tikzpicture}
\node [redbox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myred}
    We have
    \begin{align*}
        E(X)=&E(X\cdot I_{X=Y})+E(X\cdot I_{X\neq Y})\\
        E(Y)=&E(Y\cdot I_{Y=X})+E(Y\cdot I_{Y\neq X})
    \end{align*}
    $E(X\cdot I_{X\neq Y}=E(Y\cdot I_{Y\neq X}=0$, because it is an integral over $E$ and $P(E)=0$.
    \end{minipage}
};
\node[redtitle, right=4pt] at (box.north west) {Proof of $E(X)=E(Y)$};
\end{tikzpicture}

\section*{Distribution of random variables}
For a random variable $X:\Omega \rightarrow \mathbb{R}$, we can replace $(\Omega,\mathcal{A},P)$ by $(\mathbb{R},\mathcal{B},P_X)$, get:

\vspace{2pt}
\begin{tikzpicture}
\node [bluebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myblue}
    The \textbf{distribution} of $X$ is the measure $P_X:\mathcal{B}\rightarrow \mathbb{R}$, defined by $$P_X(B)=P(X^{-1}(B))=\Pr(X\in B)$$
    
    The \textbf{joint distribution} of a vector of random variables $X=(X_1,\cdots,X_N)^T$ is defined as
    $$P_X(C)=P(X^{-1}(C)) =\Pr(X\in C) $$
    where $C\in \mathcal{B}^N$, $\mathcal{B}^N$ is the Borel $\sigma-$field in $\mathbb{R}^N$, generated by the sets $B_1\times\cdots\times B_N=\left\{(x_1,\cdots,x_N)\mid x_i\in B_i,B_i\in \mathcal{B} \right\}$
    \end{minipage}
};
\node[bluetitle, right=4pt] at (box.north west) {Distribution of $X$};
\end{tikzpicture}

\subsection*{Distribution function}
For the probability space of random variable $X$: $(\mathbb{R},\mathcal{B},P_X)$, $P_X$ is determined by assigning probabilities to $\mathcal{B}$'s generating class $\mathcal{E}-\left\{\left(-\infty,x\right]\mid x\in\mathbb{R}\right\}$, hence

\vspace{2pt}
\begin{tikzpicture}
\node [bluebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myblue}
    The \textbf{distribution function} $F_X$ of $X$ is defined as $$F_X(x)=P_X\left(\left(-\infty,x\right]\right)$$
    This is just the CDF. $F_X:\mathbb{R}\rightarrow[0,1]$ and uniquely determines $P_X$. We can also write it as $F_X(x)=E\left(I_{X\leq x}\right)$.
    \end{minipage}
};
\node[bluetitle, right=4pt] at (box.north west) {Distribution function $F_X(x)$};
\end{tikzpicture}

\vspace{2pt}
\begin{tikzpicture}
\node [orangebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myorange}
    $F_X$ has the following properties:
    \begin{itemize}
        \item[(a)] \textbf{Non-decreasing}: $$x\geq y\Rightarrow F_X(x)\geq F_X(y)$$
        \textbf{Proof}: $x\geq y\Rightarrow (-\infty,y]\subseteq (-\infty,x]\Rightarrow F_X(y)=P_X((-\infty,y])\leq F_X(x)=P_X((\infty,x])$
        \item[(b)] \textbf{Continuous from the right}: $$ \lim_{y\rightarrow x+}F_X(y)=F_X(x)$$
        \textbf{Proof}: $F_X\left(x+1/n\right)=E\left( I_{X\leq x+1/n}\right)$. $I_{X\leq x+1/n}\leq 1$, $\lim_{n\rightarrow \infty}I_{x\leq x+1/n}=I_{x\leq x}$, by dominance convergence, $\lim_{n\rightarrow\infty}F_X\left(x+1/n\right)= F_X(x)$. Same logic gives $\Pr(X<x)=\lim_{y\rightarrow x-}F_X(y)=F_X(x-)$.
    \item[(c)] If $\Pr(X=\infty)=\Pr(X=-\infty)=0$ then, $\lim_{x\rightarrow-\infty}F_X(x)=0$,$\lim_{x\rightarrow \infty}F_X(x)=1$.
    
    \textbf{Proof}: use $\lim_{n\rightarrow\infty}I_{X\leq-n}=0$.
    \end{itemize}
    \end{minipage}
};
\node[orangetitle, right=4pt] at (box.north west) {Properties of $F_X$};
\end{tikzpicture}
The properties give some facts about the continuity of $F_X$ and probability of points:
\begin{itemize}
    \item[-] $F_X$ is always right continuous, but \textbf{not always continuous}.
    \item[-] Since $I_{X=x}=\lim_{n\rightarrow\infty}I_{x-1/n<X\leq x}$, we have $\Pr(X=x)=\lim_{n\rightarrow\infty}\left(F_X(x)-F_X(x-1/n\right))=F_X(x)-F_X(x-)$.
    \item[-] If $\Pr(X=x)>0$, $F_X$ is \textbf{discontinuous} in $x$, the jump at $x$ is $\Pr(X=x)$.
\end{itemize}

\section*{Function of random variable}
With a Borel measurable function $f:\mathbb{R}\rightarrow\mathbb{R}$, $Y=f\circ X:\Omega \rightarrow \mathbb{R}$ is a random variable, i.e., Borel measurable.

\vspace{2pt}
\begin{tikzpicture}
\node [bluebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myblue}
    $Y:\mathbb{R}\rightarrow \mathbb{R}$ is defined on $(\mathbb{R},\mathcal{B},P_X$, hence:
    \begin{align*}
        \Pr(Y\in B) & =P_X(\{x|f(x)\in B\})\\
        & =P(\{\omega|f(X(\omega))\in B\})
    \end{align*}
    
    and $Y$ is a simple function on $\mathbb{R}$ and $\Omega$:
    $$Y=\sum^n_{i=1}\alpha_i I_{B_i}(x)=\sum^n_{i=1}\alpha_i I_{X^{-1}(B_i)}(\omega)$$
    where $\left\{B_i\right\}$ is partition of $\mathbb{R}$, $\left\{X^{-1}(B_i)\right\}$ is hence a partition of $\Omega$.
    \end{minipage}
};
\node[bluetitle, right=4pt] at (box.north west) {distribution of $Y=f\circ X$};
\end{tikzpicture}

\vspace{2pt}
\begin{tikzpicture}
\node [bluebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myblue}
    The expectation of $Y=f\circ X$ is
    \begin{align*}
        E(Y) &= \int_{\mathbb{R}}f(x)\mathrm{d}P_X(x)=\sum^n_{i=1}\alpha_i P_X(B_i)\\
        &=\sum^n_{i=1}\alpha_i P(X^{-1}(B_i))\\
        &=\int_{\Omega}f(X(\omega))\mathrm{d}P(\omega)=E\left(f(X)\right)
    \end{align*}
    
    It is in general true that: integral of $f$ on $(\mathbb{R},\mathcal{B},P_X)$ is equal to integral of $f(X)$ on $(\Omega,\mathcal{A},P)$.
    
    Since $F_X(x)$ and $P_X(x)$ both determine the distribution of $X$, $\int_{\mathbb{R}}f(x)dP_X=\int_{\mathbb{R}}f(x)dF_X$.
    \end{minipage}
};
\node[bluetitle, right=4pt] at (box.north west) {expectation of $Y=f\circ X$};
\end{tikzpicture}

\subsection*{Absolute continuity of $P$}
The goal: to calculate probability by \textbf{summation} $P=\sum_iI_A(x_i)f(x_i)$ or \textbf{integration} $P=\int I_A(x)f(x)dx$.

We start by finding a measure $\mu$ for $P$, $\mu$ and $P$ are both defined on $(\Omega,\mathcal{A})$, and $\mu$ is easier to compute.

\vspace{2pt}
\begin{tikzpicture}
\node [bluebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myblue}
    \begin{itemize}
        \item[\textbf{1}] $\forall E \in \mathcal{A},\mu(E)\geq 0$
        \item[\textbf{2}] $\mu(\varnothing)=0$ (instead of $P(\Omega)=1$)
        \item[\textbf{3}] If $E_1,E_2,\cdots$ are pairwise disjoint (and countable), $\mu\left(\bigcup^{\infty}_{i=1}E_i\right)=\sum^{\infty}_{i=1}\mu(E_i)$
        \item[\textbf{*}] $\mu$ is \textbf{$\sigma-$finite}: $\exists \{A_i\}^\infty$, a countable partition of $\Omega$, with $\mu(A_i)<\infty$. \textbf{Probability measure $P$ is always $\sigma-$finite} since $P(\Omega)=1$
    \end{itemize}
    \end{minipage}
};
\node[bluetitle, right=4pt] at (box.north west) {Requirement of $\mu$};
\end{tikzpicture}

The most important condition of this \textit{transfer} of measure is \textbf{absolutely continuous}:

\vspace{2pt}
\begin{tikzpicture}
\node [ibluebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{white}
    For $P,\mu$ defined on $\mathcal{A}$, $P$ is \textbf{absolutely continuous} w.r.t. $\mu$ if $$\mu(A)=0\Rightarrow P(A)=0$$
    
    $\mu$ is the dominating measure, $P\ll \mu$
    \end{minipage}
};
\node[ibluetitle, right=4pt] at (box.north west) {$P$ is absolutely continuous};
\end{tikzpicture}

Now, we want to show $P\ll \mu\Rightarrow\int I_A\mathrm{d}P=\int I_Af(\omega)d\mu$, where $f$ is the density of $P$ w.r.t. $\mu$. We need the following theorem:

\vspace{2pt}
\begin{tikzpicture}
\node [ibluebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{white}
    For probability measure $P$ and measure $\mu$ defined both on $(\Omega, \mathcal{A})$, $\exists N$ with $\mu(N)=0$ and a \textbf{non-negative Borel measurable} $f:\Omega\rightarrow \mathbb{R}$, s.t. for \textbf{non-negative Borel measurable} $g:\Omega\rightarrow\mathbb{R}$, we have:
    \begin{align*}
    \int_{\Omega}g(\omega)\mathrm{d}P(\omega) = & \int_{\Omega}\underset{=0 \textit{ if }P\ll\mu}{\underline{g(\omega)I_N(\omega)\mathrm{d}P(\omega)}}\\
    & +\int_{\omega}g(\omega)f(\omega)\mathrm{d}\mu(\omega)
    \end{align*}
    
    And for any other $\tilde{N}$ and $\tilde{f}$ also satisfy this, they must be the \textit{same} w.r.t. $P$ and $\mu$:
    \begin{align*}
        P(N\setminus\tilde{N})  =P(\tilde{N}\setminus N)&=0\\
        \mu(N\setminus\tilde{N})  =\mu(\tilde{N}\setminus N)&=0\\
        P(\{x\mid f(x)\neq \tilde{f}(x)\}) = &\\ \mu(\{x\mid f(x)\neq \tilde{f}(x)\}) & =0
    \end{align*}
    \end{minipage}
};
\node[ibluetitle, right=4pt] at (box.north west) {Theorem: measure change $P$ to $\mu$};
\end{tikzpicture}

The idea is to split up the integral into two regions: $N$ and $N^C$. \textbf{Absolute continuity} gives the integral on $N$ is 0, \textbf{the theorem} gives that the integral on $N^C$ w.r.t. $P$ can be changed into an integral w.r.t. $\mu$ with the density $f$.

Two important target measures are \textbf{counting measure} and \textbf{Lebesgue measure}. They are both defined on Borel $\sigma-$field $\mathcal{B}$, hence the outcome space must be $\mathbb{R}$.

\vspace{2pt}
\begin{tikzpicture}
\node [iorangebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{white}
    For a probability $P_X$ on the space $(\mathbb{R},\mathcal{B},P_X)$, two most common dominating measures are:
    \begin{itemize}
        \item[-] \textbf{Lebesgue measure} $m$ for \textbf{continuous} $X$: for $P_X$ that assigns probability 0 to countable sets of \textbf{points} in $\mathbb{R}$. 
        \item[-] \textbf{counting measure} $\nu$ for \textbf{discrete} $X$: for $P_X$ that assigns probability 0 to sets in $\mathcal{B}$ that \textbf{do NOT contain any of the outcomes of the countable image of $X$}. 
        
        If the image of $X$ is the integers $\mathcal{I}$, the for $P_X$ assigning probability 0 to sets in $\mathcal{R}$ that do NOT containing integers, $\nu(B)=\#B\cap\mathcal{I}$ can be used.
        
        \item[-] \textbf{Mixed measure} $m+\nu$ for mixed \textbf{discrete-continuous} $X$: image of $X$ is the union of an interval and a countable set, $P_X(B)=0$ when $B$ is neither an interval nor the target countable set (integers, etc.).
    \end{itemize}
    \end{minipage}
};
\node[iorangetitle, right=4pt] at (box.north west) {Two dominating measures: $m$ and $\nu$};
\end{tikzpicture}

With dominating measures defined, we have the integrals as

\vspace{2pt}
\begin{tikzpicture}
\node [bluebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myblue}
    For the three cases, we have:
    \begin{itemize}
        \item[(a)] $P_X\ll m$: $$P_X(B)=\int_{\mathbb{R}}I_B(x)f_X(x)\mathrm{d}x$$
        \item[(b)] $P_X\ll \nu$:
        \begin{align*}
            P_X(B) &=\int_{\mathbb{R}}I_B(x)f_X(x)\mathrm{d}\nu(x)\\
            &=\sum_{B\cap\mathcal{I}}f_X(i)=\sum_{B\cap\mathcal{I}}P_X(\{i\})\\
            & =\sum_{B\cap\mathcal{I}}\Pr(X=i)
        \end{align*}
        \item[(c)] $P_X\ll m+\nu$:
        \begin{align*}
            P_X(B)=&\int_{\mathbb{R}}I_B(x)f_X(x)\mathrm{d}(m+\nu)\\
            =&\int_{\mathbb{R}}I_B(x)f_X(x)\mathrm{d}x \\
            &+\int_{\mathbb{R}}I_B(x)f_X(x)\mathrm{d}\nu
        \end{align*}
    \end{itemize}
    Where $f_X(x)=\Pr(X=x)$
    \end{minipage}
};
\node[bluetitle, right=4pt] at (box.north west) {For $P_X=\int I_B(x)\mathrm{d}P(x)$};
\end{tikzpicture}

For the density function $f_X(x):\mathbb{R}\rightarrow\mathbb{R}$ defined on $(\mathbb{R},\mathcal{B})$, it is actually a \textbf{Borel measurable random variable}.

\vspace{2pt}
\begin{tikzpicture}
\node [orangebox] (box){%
    \begin{minipage}{0.18\textwidth}
    \color{myorange}
    density function $f_X(x)$ has the following properties:
    \begin{itemize}
        \item[-] non-negative: $\forall x\in\mathbb{R},f_X(x)\geq 0$
        \item[-] integrate to 1
        If $X$ is continuous: $$1=P_X\left((-\infty,\infty)\right)=\int_{\mathbb{R}}f_X(x)\mathrm{d}x$$
        
        If $X$ is discrete:
            \begin{align*}
                1=&P_X\left((-\infty,\infty)\right)=\int_{\mathbb{R}}f_X(x)\mathrm{d}\nu\\
                =&\sum^{\infty}_{i=-\infty}f_X(i)=1
            \end{align*}
    \end{itemize}
    \end{minipage}
};
\node[orangetitle, right=4pt] at (box.north west) {Properties of $f_X(x)$};
\end{tikzpicture}

How can we find $f_X(x)=\Pr(X)=x$?

\end{multicols*}

\end{document}